[package]
name = "taurscribe"
version = "0.1.0"
description = "A Tauri App"
authors = ["you"]
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[lib]
# The `_lib` suffix may seem redundant but it is necessary
# to make the lib name unique and wouldn't conflict with the bin name.
# This seems to be only an issue on Windows, see https://github.com/rust-lang/cargo/issues/8519
name = "taurscribe_lib"
crate-type = ["staticlib", "cdylib", "rlib"]

[build-dependencies]
tauri-build = { version = "2", features = [] }

[dependencies]
tauri = { version = "2", features = ["tray-icon", "image-png"] }
tauri-plugin-opener = "2"
tauri-plugin-fs = "2"                                            # For accessing AppData directories
serde = { version = "1", features = ["derive"] }
serde_json = "1"
cpal = "0.15"
crossbeam-channel = "0.5"
hound = "3.5"
chrono = "0.4"
# whisper-rs with CUDA + Vulkan (includes 12x iGPU performance boost from whisper.cpp v1.8.3)
# Update periodically: cargo update -p whisper-rs
# NOTE: Disabled CUDA/Vulkan for now - enable if you have CUDA Toolkit and Vulkan SDK installed
# --- General Dependencies (Required on ALL platforms) ---
rubato = "0.14"
dirs = "6.0.0"
rdev = "0.5"
regex = "1.12.2"
tauri-plugin-store = "2.4.2"
enigo = "0.6.1"
# Candle ML Framework (CUDA baseline; per-platform overrides below, e.g. Metal on macOS)
candle-core = { git = "https://github.com/huggingface/candle.git", version = "0.9.2", features = ["cuda"] }
candle-nn = { git = "https://github.com/huggingface/candle.git", version = "0.9.2", features = ["cuda"] }
candle-transformers = { git = "https://github.com/huggingface/candle.git", version = "0.9.2", features = ["cuda"] }
tokenizers = "0.21.0"
hf-hub = { version = "0.4", default-features = false, features = [
    "native-tls",
    "tokio",
] }
anyhow = "1.0"

# SymSpell - Fast spell checking
symspell = "0.4"

# File system watcher - Detect model changes
notify = "6.1"

# --- Platform-Specific AI Backends ---


# 1. Base / CPU Fallback (ARM Windows/Linux)
# -------------------------------------------------------------------------
whisper-rs = { git = "https://codeberg.org/tazz4843/whisper-rs.git" }
parakeet-rs = { version = "=0.3.0" }
ort = { version = "2.0.0-rc.11", features = ["download-binaries"] }
# Downloader & Verification
futures-util = "0.3.31"
reqwest = { version = "0.13.1", default-features = false, features = [
    "native-tls",
    "stream",
    "json",
    "multipart",
] }
sha2 = "0.10.9"
hex = "0.4.3"
sha1 = "0.10.6"
# Candle (Hugging Face) - Machine Learning Framework


# 2. macOS Configuration
# -------------------------------------------------------------------------

# 2. macOS Configuration (ARM & Intel)
# CoreML: Hardware-accelerated on Apple Silicon (M1/M2/M3/M4)
# XNNPACK: Cross-platform CPU optimization (fallback for Intel Macs)
# -------------------------------------------------------------------------
[target.'cfg(target_os = "macos")'.dependencies]
whisper-rs = { git = "https://codeberg.org/tazz4843/whisper-rs.git" }
ort = { version = "2.0.0-rc.11", features = ["download-binaries", "xnnpack"] }
# Candle with Metal for LLM GPU acceleration on Apple Silicon
candle-core = { git = "https://github.com/huggingface/candle.git", version = "0.9.2", features = [
    "metal",
] }

# 3. Windows (x86_64) -> CUDA + DirectML + TensorRT
# CUDA: NVIDIA GPU acceleration (primary)
# DirectML: Universal Windows GPU acceleration (AMD/Intel fallback)
# TensorRT: NVIDIA optimized inference
# WebGPU: Cross-platform GPU (additional fallback)
# XNNPACK: CPU optimization
# -------------------------------------------------------------------------
[target.'cfg(all(target_os = "windows", target_arch = "x86_64"))'.dependencies]


whisper-rs = { git = "https://codeberg.org/tazz4843/whisper-rs.git", features = [
    "cuda",
    "vulkan",
] }
parakeet-rs = { version = "=0.3.0", features = ["cuda", "directml"] }
ort = { version = "2.0.0-rc.11", features = [
    "download-binaries",
    "cuda",
    "directml",
    "tensorrt",
    "xnnpack",
] }
# Candle with CUDA for LLM GPU acceleration (core + nn + transformers need cuda for RMSNorm etc.)
candle-core = { git = "https://github.com/huggingface/candle.git", version = "0.9.2", features = [
    "cuda",
] }
candle-nn = { git = "https://github.com/huggingface/candle.git", version = "0.9.2", features = [
    "cuda",
] }
candle-transformers = { git = "https://github.com/huggingface/candle.git", version = "0.9.2", features = [
    "cuda",
] }
# 3b. Linux (x86_64) -> CUDA + TensorRT
# CUDA: NVIDIA GPU acceleration (primary)
# TensorRT: NVIDIA optimized inference
# WebGPU: Cross-platform GPU (AMD/Intel fallback)
# XNNPACK: CPU optimization
# -------------------------------------------------------------------------
[target.'cfg(all(target_os = "linux", target_arch = "x86_64"))'.dependencies]


whisper-rs = { git = "https://codeberg.org/tazz4843/whisper-rs.git", features = [
    "cuda",
    "vulkan",
] }
parakeet-rs = { version = "=0.3.0", features = ["cuda"] }
ort = { version = "2.0.0-rc.11", features = [
    "download-binaries",
    "cuda",
    "tensorrt",
    "xnnpack",
] }
# Candle with CUDA for LLM GPU acceleration (core + nn + transformers need cuda for RMSNorm etc.)
candle-core = { git = "https://github.com/huggingface/candle.git", version = "0.9.2", features = [
    "cuda",
] }
candle-nn = { git = "https://github.com/huggingface/candle.git", version = "0.9.2", features = [
    "cuda",
] }
candle-transformers = { git = "https://github.com/huggingface/candle.git", version = "0.9.2", features = [
    "cuda",
] }
# 4. Windows (ARM64) -> DirectML + XNNPACK
# DirectML: Hardware-accelerated on Qualcomm Snapdragon X ARM processors
# XNNPACK: Cross-platform CPU optimization
# -------------------------------------------------------------------------
[target.'cfg(all(target_os = "windows", target_arch = "aarch64"))'.dependencies]
ort = { version = "2.0.0-rc.11", features = [
    "download-binaries",
    "directml",
    "xnnpack",
] }
parakeet-rs = { version = "=0.3.0", features = ["directml"] }

# 5. Linux (ARM64) -> XNNPACK
# XNNPACK: CPU optimization for ARM processors (Raspberry Pi, etc.)
# -------------------------------------------------------------------------
[target.'cfg(all(target_os = "linux", target_arch = "aarch64"))'.dependencies]
whisper-rs = { git = "https://codeberg.org/tazz4843/whisper-rs.git" }
parakeet-rs = { version = "=0.3.0" }
ort = { version = "2.0.0-rc.11", features = ["download-binaries", "xnnpack"] }

# Speed up dev builds by optimizing dependencies but not your code
[profile.dev]
opt-level = 0 # Don't optimize your code (fast builds)

[profile.dev.package."*"]
opt-level = 3
debug-assertions = false
overflow-checks = false

# If you want even faster dev builds, uncomment this (but slower runtime):
# [profile.dev.package."*"]
# opt-level = 0
